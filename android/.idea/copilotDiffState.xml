<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/app/src/main/java/com/google/mediapipe/examples/gesturerecognizer/GestureRecognizerHelper.kt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app/src/main/java/com/google/mediapipe/examples/gesturerecognizer/GestureRecognizerHelper.kt" />
              <option name="originalContent" value="/*&#10; * Copyright 2022 The TensorFlow Authors. All Rights Reserved.&#10; *&#10; * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);&#10; * you may not use this file except in compliance with the License.&#10; * You may obtain a copy of the License at&#10; *&#10; *             http://www.apache.org/licenses/LICENSE-2.0&#10; *&#10; * Unless required by applicable law or agreed to in writing, software&#10; * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&#10; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#10; * See the License for the specific language governing permissions and&#10; * limitations under the License.&#10; */&#10;package com.google.mediapipe.examples.gesturerecognizer&#10;&#10;import android.content.Context&#10;import android.graphics.Bitmap&#10;import android.graphics.Matrix&#10;import android.media.MediaMetadataRetriever&#10;import android.net.Uri&#10;import android.os.SystemClock&#10;import android.util.Log&#10;import androidx.annotation.VisibleForTesting&#10;import androidx.camera.core.ImageProxy&#10;import com.google.mediapipe.framework.image.BitmapImageBuilder&#10;import com.google.mediapipe.framework.image.MPImage&#10;import com.google.mediapipe.tasks.core.BaseOptions&#10;import com.google.mediapipe.tasks.core.Delegate&#10;import com.google.mediapipe.tasks.vision.core.RunningMode&#10;import com.google.mediapipe.tasks.vision.gesturerecognizer.GestureRecognizer&#10;import com.google.mediapipe.tasks.vision.gesturerecognizer.GestureRecognizerResult&#10;&#10;class GestureRecognizerHelper(&#10;    var minHandDetectionConfidence: Float = DEFAULT_HAND_DETECTION_CONFIDENCE,&#10;    var minHandTrackingConfidence: Float = DEFAULT_HAND_TRACKING_CONFIDENCE,&#10;    var minHandPresenceConfidence: Float = DEFAULT_HAND_PRESENCE_CONFIDENCE,&#10;    var currentDelegate: Int = DELEGATE_CPU,&#10;    var runningMode: RunningMode = RunningMode.IMAGE,&#10;    val context: Context,&#10;    val gestureRecognizerListener: GestureRecognizerListener? = null&#10;) {&#10;&#10;    // For this example this needs to be a var so it can be reset on changes. If the GestureRecognizer&#10;    // will not change, a lazy val would be preferable.&#10;    private var gestureRecognizer: GestureRecognizer? = null&#10;&#10;    init {&#10;        setupGestureRecognizer()&#10;    }&#10;&#10;    fun clearGestureRecognizer() {&#10;        gestureRecognizer?.close()&#10;        gestureRecognizer = null&#10;    }&#10;&#10;    // Initialize the gesture recognizer using current settings on the&#10;    // thread that is using it. CPU can be used with recognizers&#10;    // that are created on the main thread and used on a background thread, but&#10;    // the GPU delegate needs to be used on the thread that initialized the recognizer&#10;    fun setupGestureRecognizer() {&#10;        // Set general recognition options, including number of used threads&#10;        val baseOptionBuilder = BaseOptions.builder()&#10;&#10;        // Use the specified hardware for running the model. Default to CPU&#10;        when (currentDelegate) {&#10;            DELEGATE_CPU -&gt; {&#10;                baseOptionBuilder.setDelegate(Delegate.CPU)&#10;            }&#10;            DELEGATE_GPU -&gt; {&#10;                baseOptionBuilder.setDelegate(Delegate.GPU)&#10;            }&#10;        }&#10;&#10;        baseOptionBuilder.setModelAssetPath(MP_RECOGNIZER_TASK)&#10;&#10;        try {&#10;            val baseOptions = baseOptionBuilder.build()&#10;            val optionsBuilder =&#10;                GestureRecognizer.GestureRecognizerOptions.builder()&#10;                    .setBaseOptions(baseOptions)&#10;                    .setMinHandDetectionConfidence(minHandDetectionConfidence)&#10;                    .setMinTrackingConfidence(minHandTrackingConfidence)&#10;                    .setMinHandPresenceConfidence(minHandPresenceConfidence)&#10;                    .setRunningMode(runningMode)&#10;&#10;            if (runningMode == RunningMode.LIVE_STREAM) {&#10;                optionsBuilder&#10;                    .setResultListener(this::returnLivestreamResult)&#10;                    .setErrorListener(this::returnLivestreamError)&#10;            }&#10;            val options = optionsBuilder.build()&#10;            gestureRecognizer =&#10;                GestureRecognizer.createFromOptions(context, options)&#10;        } catch (e: IllegalStateException) {&#10;            gestureRecognizerListener?.onError(&#10;                &quot;Gesture recognizer failed to initialize. See error logs for &quot; + &quot;details&quot;&#10;            )&#10;            Log.e(&#10;                TAG,&#10;                &quot;MP Task Vision failed to load the task with error: &quot; + e.message&#10;            )&#10;        } catch (e: RuntimeException) {&#10;            gestureRecognizerListener?.onError(&#10;                &quot;Gesture recognizer failed to initialize. See error logs for &quot; + &quot;details&quot;,&#10;                GPU_ERROR&#10;            )&#10;            Log.e(&#10;                TAG,&#10;                &quot;MP Task Vision failed to load the task with error: &quot; + e.message&#10;            )&#10;        }&#10;    }&#10;&#10;    // Convert the ImageProxy to MP Image and feed it to GestureRecognizer.&#10;    fun recognizeLiveStream(&#10;        imageProxy: ImageProxy,&#10;    ) {&#10;        val frameTime = SystemClock.uptimeMillis()&#10;&#10;        // Copy out RGB bits from the frame to a bitmap buffer&#10;        val bitmapBuffer = Bitmap.createBitmap(&#10;            imageProxy.width, imageProxy.height, Bitmap.Config.ARGB_8888&#10;        )&#10;        imageProxy.use { bitmapBuffer.copyPixelsFromBuffer(imageProxy.planes[0].buffer) }&#10;        imageProxy.close()&#10;&#10;        val matrix = Matrix().apply {&#10;            // Rotate the frame received from the camera to be in the same direction as it'll be shown&#10;            postRotate(imageProxy.imageInfo.rotationDegrees.toFloat())&#10;&#10;            // flip image since we only support front camera&#10;            postScale(&#10;                -1f, 1f, imageProxy.width.toFloat(), imageProxy.height.toFloat()&#10;            )&#10;        }&#10;&#10;        // Rotate bitmap to match what our model expects&#10;        val rotatedBitmap = Bitmap.createBitmap(&#10;            bitmapBuffer,&#10;            0,&#10;            0,&#10;            bitmapBuffer.width,&#10;            bitmapBuffer.height,&#10;            matrix,&#10;            true&#10;        )&#10;&#10;        // Convert the input Bitmap object to an MPImage object to run inference&#10;        val mpImage = BitmapImageBuilder(rotatedBitmap).build()&#10;&#10;        recognizeAsync(mpImage, frameTime)&#10;    }&#10;&#10;    // Run hand gesture recognition using MediaPipe Gesture Recognition API&#10;    @VisibleForTesting&#10;    fun recognizeAsync(mpImage: MPImage, frameTime: Long) {&#10;        // As we're using running mode LIVE_STREAM, the recognition result will&#10;        // be returned in returnLivestreamResult function&#10;        gestureRecognizer?.recognizeAsync(mpImage, frameTime)&#10;    }&#10;&#10;    // Accepts the URI for a video file loaded from the user's gallery and attempts to run&#10;    // gesture recognizer inference on the video. This process will evaluate&#10;    // every frame in the video and attach the results to a bundle that will be&#10;    // returned.&#10;    fun recognizeVideoFile(&#10;        videoUri: Uri,&#10;        inferenceIntervalMs: Long&#10;    ): ResultBundle? {&#10;        if (runningMode != RunningMode.VIDEO) {&#10;            throw IllegalArgumentException(&#10;                &quot;Attempting to call recognizeVideoFile&quot; +&#10;                        &quot; while not using RunningMode.VIDEO&quot;&#10;            )&#10;        }&#10;&#10;        // Inference time is the difference between the system time at the start and finish of the&#10;        // process&#10;        val startTime = SystemClock.uptimeMillis()&#10;&#10;        var didErrorOccurred = false&#10;&#10;        // Load frames from the video and run the gesture recognizer.&#10;        val retriever = MediaMetadataRetriever()&#10;        retriever.setDataSource(context, videoUri)&#10;        val videoLengthMs =&#10;            retriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_DURATION)&#10;                ?.toLong()&#10;&#10;        // Note: We need to read width/height from frame instead of getting the width/height&#10;        // of the video directly because MediaRetriever returns frames that are smaller than the&#10;        // actual dimension of the video file.&#10;        val firstFrame = retriever.getFrameAtTime(0)&#10;        val width = firstFrame?.width&#10;        val height = firstFrame?.height&#10;&#10;        // If the video is invalid, returns a null recognition result&#10;        if ((videoLengthMs == null) || (width == null) || (height == null)) return null&#10;&#10;        // Next, we'll get one frame every frameInterval ms, then run recognizer&#10;        // on these frames.&#10;        val resultList = mutableListOf&lt;GestureRecognizerResult&gt;()&#10;        val numberOfFrameToRead = videoLengthMs.div(inferenceIntervalMs)&#10;&#10;        for (i in 0..numberOfFrameToRead) {&#10;            val timestampMs = i * inferenceIntervalMs // ms&#10;&#10;            retriever&#10;                .getFrameAtTime(&#10;                    timestampMs * 1000, // convert from ms to micro-s&#10;                    MediaMetadataRetriever.OPTION_CLOSEST&#10;                )&#10;                ?.let { frame -&gt;&#10;                    // Convert the video frame to ARGB_8888 which is required by the MediaPipe&#10;                    val argb8888Frame =&#10;                        if (frame.config == Bitmap.Config.ARGB_8888) frame&#10;                        else frame.copy(Bitmap.Config.ARGB_8888, false)&#10;&#10;                    // Convert the input Bitmap object to an MPImage object to run inference&#10;                    val mpImage = BitmapImageBuilder(argb8888Frame).build()&#10;&#10;                    // Run gesture recognizer using MediaPipe Gesture Recognizer&#10;                    // API&#10;                    gestureRecognizer?.recognizeForVideo(mpImage, timestampMs)&#10;                        ?.let { recognizerResult -&gt;&#10;                            resultList.add(recognizerResult)&#10;                        } ?: {&#10;                        didErrorOccurred = true&#10;                        gestureRecognizerListener?.onError(&#10;                            &quot;ResultBundle could not be returned&quot; +&#10;                                    &quot; in recognizeVideoFile&quot;&#10;                        )&#10;                    }&#10;                }&#10;                ?: run {&#10;                    didErrorOccurred = true&#10;                    gestureRecognizerListener?.onError(&#10;                        &quot;Frame at specified time could not be&quot; +&#10;                                &quot; retrieved when recognition in video.&quot;&#10;                    )&#10;                }&#10;        }&#10;&#10;        retriever.release()&#10;&#10;        val inferenceTimePerFrameMs =&#10;            (SystemClock.uptimeMillis() - startTime).div(numberOfFrameToRead)&#10;&#10;        return if (didErrorOccurred) {&#10;            null&#10;        } else {&#10;            ResultBundle(resultList, inferenceTimePerFrameMs, height, width)&#10;        }&#10;    }&#10;&#10;    // Accepted a Bitmap and runs gesture recognizer inference on it to&#10;    // return results back to the caller&#10;    fun recognizeImage(image: Bitmap): ResultBundle? {&#10;        if (runningMode != RunningMode.IMAGE) {&#10;            throw IllegalArgumentException(&#10;                &quot;Attempting to call detectImage&quot; +&#10;                        &quot; while not using RunningMode.IMAGE&quot;&#10;            )&#10;        }&#10;&#10;&#10;        // Inference time is the difference between the system time at the&#10;        // start and finish of the process&#10;        val startTime = SystemClock.uptimeMillis()&#10;&#10;        // Convert the input Bitmap object to an MPImage object to run inference&#10;        val mpImage = BitmapImageBuilder(image).build()&#10;&#10;        // Run gesture recognizer using MediaPipe Gesture Recognizer API&#10;        gestureRecognizer?.recognize(mpImage)?.also { recognizerResult -&gt;&#10;            val inferenceTimeMs = SystemClock.uptimeMillis() - startTime&#10;            return ResultBundle(&#10;                listOf(recognizerResult),&#10;                inferenceTimeMs,&#10;                image.height,&#10;                image.width&#10;            )&#10;        }&#10;&#10;        // If gestureRecognizer?.recognize() returns null, this is likely an error. Returning null&#10;        // to indicate this.&#10;        gestureRecognizerListener?.onError(&#10;            &quot;Gesture Recognizer failed to recognize.&quot;&#10;        )&#10;        return null&#10;    }&#10;&#10;    // Return running status of the recognizer helper&#10;    fun isClosed(): Boolean {&#10;        return gestureRecognizer == null&#10;    }&#10;&#10;    // Return the recognition result to the GestureRecognizerHelper's caller&#10;    private fun returnLivestreamResult(&#10;        result: GestureRecognizerResult, input: MPImage&#10;    ) {&#10;        val finishTimeMs = SystemClock.uptimeMillis()&#10;        val inferenceTime = finishTimeMs - result.timestampMs()&#10;&#10;        gestureRecognizerListener?.onResults(&#10;            ResultBundle(&#10;                listOf(result), inferenceTime, input.height, input.width&#10;            )&#10;        )&#10;    }&#10;&#10;    // Return errors thrown during recognition to this GestureRecognizerHelper's&#10;    // caller&#10;    private fun returnLivestreamError(error: RuntimeException) {&#10;        gestureRecognizerListener?.onError(&#10;            error.message ?: &quot;An unknown error has occurred&quot;&#10;        )&#10;    }&#10;&#10;    companion object {&#10;        val TAG = &quot;GestureRecognizerHelper ${this.hashCode()}&quot;&#10;        private const val MP_RECOGNIZER_TASK = &quot;gesture_recognizer.task&quot;&#10;&#10;        const val DELEGATE_CPU = 0&#10;        const val DELEGATE_GPU = 1&#10;        const val DEFAULT_HAND_DETECTION_CONFIDENCE = 0.5F&#10;        const val DEFAULT_HAND_TRACKING_CONFIDENCE = 0.5F&#10;        const val DEFAULT_HAND_PRESENCE_CONFIDENCE = 0.5F&#10;        const val OTHER_ERROR = 0&#10;        const val GPU_ERROR = 1&#10;    }&#10;&#10;    data class ResultBundle(&#10;        val results: List&lt;GestureRecognizerResult&gt;,&#10;        val inferenceTime: Long,&#10;        val inputImageHeight: Int,&#10;        val inputImageWidth: Int,&#10;    )&#10;&#10;    interface GestureRecognizerListener {&#10;        fun onError(error: String, errorCode: Int = OTHER_ERROR)&#10;        fun onResults(resultBundle: ResultBundle)&#10;    }&#10;}&#10;" />
              <option name="updatedContent" value="/*&#10; * Copyright 2022 The TensorFlow Authors. All Rights Reserved.&#10; *&#10; * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);&#10; * you may not use this file except in compliance with the License.&#10; * You may obtain a copy of the License at&#10; *&#10; *             http://www.apache.org/licenses/LICENSE-2.0&#10; *&#10; * Unless required by applicable law or agreed to in writing, software&#10; * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&#10; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#10; * See the License for the specific language governing permissions and&#10; * limitations under the License.&#10; */&#10;package com.google.mediapipe.examples.gesturerecognizer&#10;&#10;import android.content.Context&#10;import android.graphics.Bitmap&#10;import android.graphics.Matrix&#10;import android.media.MediaMetadataRetriever&#10;import android.net.Uri&#10;import android.os.SystemClock&#10;import android.util.Log&#10;import androidx.annotation.VisibleForTesting&#10;import androidx.camera.core.ImageProxy&#10;import com.google.mediapipe.framework.image.BitmapImageBuilder&#10;import com.google.mediapipe.framework.image.MPImage&#10;import com.google.mediapipe.tasks.core.BaseOptions&#10;import com.google.mediapipe.tasks.core.Delegate&#10;import com.google.mediapipe.tasks.vision.core.RunningMode&#10;import com.google.mediapipe.tasks.vision.gesturerecognizer.GestureRecognizer&#10;import com.google.mediapipe.tasks.vision.gesturerecognizer.GestureRecognizerResult&#10;import com.google.mediapipe.tasks.components.containers.NormalizedLandmark&#10;import kotlin.math.acos&#10;import kotlin.math.pow&#10;import kotlin.math.sqrt&#10;&#10;class GestureRecognizerHelper(&#10;    var minHandDetectionConfidence: Float = DEFAULT_HAND_DETECTION_CONFIDENCE,&#10;    var minHandTrackingConfidence: Float = DEFAULT_HAND_TRACKING_CONFIDENCE,&#10;    var minHandPresenceConfidence: Float = DEFAULT_HAND_PRESENCE_CONFIDENCE,&#10;    var currentDelegate: Int = DELEGATE_CPU,&#10;    var runningMode: RunningMode = RunningMode.IMAGE,&#10;    val context: Context,&#10;    val gestureRecognizerListener: GestureRecognizerListener? = null&#10;) {&#10;&#10;    // For this example this needs to be a var so it can be reset on changes. If the GestureRecognizer&#10;    // will not change, a lazy val would be preferable.&#10;    private var gestureRecognizer: GestureRecognizer? = null&#10;&#10;    init {&#10;        setupGestureRecognizer()&#10;    }&#10;&#10;    fun clearGestureRecognizer() {&#10;        gestureRecognizer?.close()&#10;        gestureRecognizer = null&#10;    }&#10;&#10;    // Initialize the gesture recognizer using current settings on the&#10;    // thread that is using it. CPU can be used with recognizers&#10;    // that are created on the main thread and used on a background thread, but&#10;    // the GPU delegate needs to be used on the thread that initialized the recognizer&#10;    fun setupGestureRecognizer() {&#10;        // Set general recognition options, including number of used threads&#10;        val baseOptionBuilder = BaseOptions.builder()&#10;&#10;        // Use the specified hardware for running the model. Default to CPU&#10;        when (currentDelegate) {&#10;            DELEGATE_CPU -&gt; {&#10;                baseOptionBuilder.setDelegate(Delegate.CPU)&#10;            }&#10;            DELEGATE_GPU -&gt; {&#10;                baseOptionBuilder.setDelegate(Delegate.GPU)&#10;            }&#10;        }&#10;&#10;        baseOptionBuilder.setModelAssetPath(MP_RECOGNIZER_TASK)&#10;&#10;        try {&#10;            val baseOptions = baseOptionBuilder.build()&#10;            val optionsBuilder =&#10;                GestureRecognizer.GestureRecognizerOptions.builder()&#10;                    .setBaseOptions(baseOptions)&#10;                    .setMinHandDetectionConfidence(minHandDetectionConfidence)&#10;                    .setMinTrackingConfidence(minHandTrackingConfidence)&#10;                    .setMinHandPresenceConfidence(minHandPresenceConfidence)&#10;                    .setRunningMode(runningMode)&#10;&#10;            if (runningMode == RunningMode.LIVE_STREAM) {&#10;                optionsBuilder&#10;                    .setResultListener(this::returnLivestreamResult)&#10;                    .setErrorListener(this::returnLivestreamError)&#10;            }&#10;            val options = optionsBuilder.build()&#10;            gestureRecognizer =&#10;                GestureRecognizer.createFromOptions(context, options)&#10;        } catch (e: IllegalStateException) {&#10;            gestureRecognizerListener?.onError(&#10;                &quot;Gesture recognizer failed to initialize. See error logs for &quot; + &quot;details&quot;&#10;            )&#10;            Log.e(&#10;                TAG,&#10;                &quot;MP Task Vision failed to load the task with error: &quot; + e.message&#10;            )&#10;        } catch (e: RuntimeException) {&#10;            gestureRecognizerListener?.onError(&#10;                &quot;Gesture recognizer failed to initialize. See error logs for &quot; + &quot;details&quot;,&#10;                GPU_ERROR&#10;            )&#10;            Log.e(&#10;                TAG,&#10;                &quot;MP Task Vision failed to load the task with error: &quot; + e.message&#10;            )&#10;        }&#10;    }&#10;&#10;    // Convert the ImageProxy to MP Image and feed it to GestureRecognizer.&#10;    fun recognizeLiveStream(&#10;        imageProxy: ImageProxy,&#10;    ) {&#10;        val frameTime = SystemClock.uptimeMillis()&#10;&#10;        // Copy out RGB bits from the frame to a bitmap buffer&#10;        val bitmapBuffer = Bitmap.createBitmap(&#10;            imageProxy.width, imageProxy.height, Bitmap.Config.ARGB_8888&#10;        )&#10;        imageProxy.use { bitmapBuffer.copyPixelsFromBuffer(imageProxy.planes[0].buffer) }&#10;        imageProxy.close()&#10;&#10;        val matrix = Matrix().apply {&#10;            // Rotate the frame received from the camera to be in the same direction as it'll be shown&#10;            postRotate(imageProxy.imageInfo.rotationDegrees.toFloat())&#10;&#10;            // flip image since we only support front camera&#10;            postScale(&#10;                -1f, 1f, imageProxy.width.toFloat(), imageProxy.height.toFloat()&#10;            )&#10;        }&#10;&#10;        // Rotate bitmap to match what our model expects&#10;        val rotatedBitmap = Bitmap.createBitmap(&#10;            bitmapBuffer,&#10;            0,&#10;            0,&#10;            bitmapBuffer.width,&#10;            bitmapBuffer.height,&#10;            matrix,&#10;            true&#10;        )&#10;&#10;        // Convert the input Bitmap object to an MPImage object to run inference&#10;        val mpImage = BitmapImageBuilder(rotatedBitmap).build()&#10;&#10;        recognizeAsync(mpImage, frameTime)&#10;    }&#10;&#10;    // Run hand gesture recognition using MediaPipe Gesture Recognition API&#10;    @VisibleForTesting&#10;    fun recognizeAsync(mpImage: MPImage, frameTime: Long) {&#10;        // As we're using running mode LIVE_STREAM, the recognition result will&#10;        // be returned in returnLivestreamResult function&#10;        gestureRecognizer?.recognizeAsync(mpImage, frameTime)&#10;    }&#10;&#10;    // Accepts the URI for a video file loaded from the user's gallery and attempts to run&#10;    // gesture recognizer inference on the video. This process will evaluate&#10;    // every frame in the video and attach the results to a bundle that will be&#10;    // returned.&#10;    fun recognizeVideoFile(&#10;        videoUri: Uri,&#10;        inferenceIntervalMs: Long&#10;    ): ResultBundle? {&#10;        if (runningMode != RunningMode.VIDEO) {&#10;            throw IllegalArgumentException(&#10;                &quot;Attempting to call recognizeVideoFile&quot; +&#10;                        &quot; while not using RunningMode.VIDEO&quot;&#10;            )&#10;        }&#10;&#10;        // Inference time is the difference between the system time at the start and finish of the&#10;        // process&#10;        val startTime = SystemClock.uptimeMillis()&#10;&#10;        var didErrorOccurred = false&#10;&#10;        // Load frames from the video and run the gesture recognizer.&#10;        val retriever = MediaMetadataRetriever()&#10;        retriever.setDataSource(context, videoUri)&#10;        val videoLengthMs =&#10;            retriever.extractMetadata(MediaMetadataRetriever.METADATA_KEY_DURATION)&#10;                ?.toLong()&#10;&#10;        // Note: We need to read width/height from frame instead of getting the width/height&#10;        // of the video directly because MediaRetriever returns frames that are smaller than the&#10;        // actual dimension of the video file.&#10;        val firstFrame = retriever.getFrameAtTime(0)&#10;        val width = firstFrame?.width&#10;        val height = firstFrame?.height&#10;&#10;        // If the video is invalid, returns a null recognition result&#10;        if ((videoLengthMs == null) || (width == null) || (height == null)) return null&#10;&#10;        // Next, we'll get one frame every frameInterval ms, then run recognizer&#10;        // on these frames.&#10;        val resultList = mutableListOf&lt;GestureRecognizerResult&gt;()&#10;        val numberOfFrameToRead = videoLengthMs.div(inferenceIntervalMs)&#10;&#10;        for (i in 0..numberOfFrameToRead) {&#10;            val timestampMs = i * inferenceIntervalMs // ms&#10;&#10;            retriever&#10;                .getFrameAtTime(&#10;                    timestampMs * 1000, // convert from ms to micro-s&#10;                    MediaMetadataRetriever.OPTION_CLOSEST&#10;                )&#10;                ?.let { frame -&gt;&#10;                    // Convert the video frame to ARGB_8888 which is required by the MediaPipe&#10;                    val argb8888Frame =&#10;                        if (frame.config == Bitmap.Config.ARGB_8888) frame&#10;                        else frame.copy(Bitmap.Config.ARGB_8888, false)&#10;&#10;                    // Convert the input Bitmap object to an MPImage object to run inference&#10;                    val mpImage = BitmapImageBuilder(argb8888Frame).build()&#10;&#10;                    // Run gesture recognizer using MediaPipe Gesture Recognizer&#10;                    // API&#10;                    gestureRecognizer?.recognizeForVideo(mpImage, timestampMs)&#10;                        ?.let { recognizerResult -&gt;&#10;                            resultList.add(recognizerResult)&#10;                        } ?: {&#10;                        didErrorOccurred = true&#10;                        gestureRecognizerListener?.onError(&#10;                            &quot;ResultBundle could not be returned&quot; +&#10;                                    &quot; in recognizeVideoFile&quot;&#10;                        )&#10;                    }&#10;                }&#10;                ?: run {&#10;                    didErrorOccurred = true&#10;                    gestureRecognizerListener?.onError(&#10;                        &quot;Frame at specified time could not be&quot; +&#10;                                &quot; retrieved when recognition in video.&quot;&#10;                    )&#10;                }&#10;        }&#10;&#10;        retriever.release()&#10;&#10;        val inferenceTimePerFrameMs =&#10;            (SystemClock.uptimeMillis() - startTime).div(numberOfFrameToRead)&#10;&#10;        return if (didErrorOccurred) {&#10;            null&#10;        } else {&#10;            ResultBundle(resultList, inferenceTimePerFrameMs, height, width)&#10;        }&#10;    }&#10;&#10;    // Accepted a Bitmap and runs gesture recognizer inference on it to&#10;    // return results back to the caller&#10;    fun recognizeImage(image: Bitmap): ResultBundle? {&#10;        if (runningMode != RunningMode.IMAGE) {&#10;            throw IllegalArgumentException(&#10;                &quot;Attempting to call detectImage&quot; +&#10;                        &quot; while not using RunningMode.IMAGE&quot;&#10;            )&#10;        }&#10;&#10;&#10;        // Inference time is the difference between the system time at the&#10;        // start and finish of the process&#10;        val startTime = SystemClock.uptimeMillis()&#10;&#10;        // Convert the input Bitmap object to an MPImage object to run inference&#10;        val mpImage = BitmapImageBuilder(image).build()&#10;&#10;        // Run gesture recognizer using MediaPipe Gesture Recognizer API&#10;        gestureRecognizer?.recognize(mpImage)?.also { recognizerResult -&gt;&#10;            val inferenceTimeMs = SystemClock.uptimeMillis() - startTime&#10;            return ResultBundle(&#10;                listOf(recognizerResult),&#10;                inferenceTimeMs,&#10;                image.height,&#10;                image.width&#10;            )&#10;        }&#10;&#10;        // If gestureRecognizer?.recognize() returns null, this is likely an error. Returning null&#10;        // to indicate this.&#10;        gestureRecognizerListener?.onError(&#10;            &quot;Gesture Recognizer failed to recognize.&quot;&#10;        )&#10;        return null&#10;    }&#10;&#10;    // Return running status of the recognizer helper&#10;    fun isClosed(): Boolean {&#10;        return gestureRecognizer == null&#10;    }&#10;&#10;    // Return the recognition result to the GestureRecognizerHelper's caller&#10;    private fun returnLivestreamResult(&#10;        result: GestureRecognizerResult, input: MPImage&#10;    ) {&#10;        val finishTimeMs = SystemClock.uptimeMillis()&#10;        val inferenceTime = finishTimeMs - result.timestampMs()&#10;&#10;        gestureRecognizerListener?.onResults(&#10;            ResultBundle(&#10;                listOf(result), inferenceTime, input.height, input.width&#10;            )&#10;        )&#10;    }&#10;&#10;    // Return errors thrown during recognition to this GestureRecognizerHelper's&#10;    // caller&#10;    private fun returnLivestreamError(error: RuntimeException) {&#10;        gestureRecognizerListener?.onError(&#10;            error.message ?: &quot;An unknown error has occurred&quot;&#10;        )&#10;    }&#10;&#10;    // Calculate angle between three joints (joint2 is the vertex)&#10;    fun calculateJointAngle(&#10;        joint1: NormalizedLandmark,&#10;        joint2: NormalizedLandmark,&#10;        joint3: NormalizedLandmark&#10;    ): Double {&#10;        // Calculate vectors from joint2 to joint1 and joint2 to joint3&#10;        val vector1 = doubleArrayOf(&#10;            (joint1.x() - joint2.x()).toDouble(),&#10;            (joint1.y() - joint2.y()).toDouble(),&#10;            (joint1.z() - joint2.z()).toDouble()&#10;        )&#10;        &#10;        val vector2 = doubleArrayOf(&#10;            (joint3.x() - joint2.x()).toDouble(),&#10;            (joint3.y() - joint2.y()).toDouble(),&#10;            (joint3.z() - joint2.z()).toDouble()&#10;        )&#10;        &#10;        // Calculate dot product&#10;        val dotProduct = vector1[0] * vector2[0] + vector1[1] * vector2[1] + vector1[2] * vector2[2]&#10;        &#10;        // Calculate magnitudes&#10;        val magnitude1 = sqrt(vector1[0].pow(2) + vector1[1].pow(2) + vector1[2].pow(2))&#10;        val magnitude2 = sqrt(vector2[0].pow(2) + vector2[1].pow(2) + vector2[2].pow(2))&#10;        &#10;        // Calculate angle in degrees&#10;        val cosAngle = dotProduct / (magnitude1 * magnitude2)&#10;        // Clamp cosAngle to [-1, 1] to avoid NaN from acos&#10;        val clampedCosAngle = cosAngle.coerceIn(-1.0, 1.0)&#10;        &#10;        return Math.toDegrees(acos(clampedCosAngle))&#10;    }&#10;&#10;    // Calculate all finger joint angles for a hand&#10;    // Returns a map with finger names and their joint angles&#10;    fun calculateFingerAngles(landmarks: List&lt;NormalizedLandmark&gt;): Map&lt;String, List&lt;Double&gt;&gt; {&#10;        if (landmarks.size &lt; 21) {&#10;            return emptyMap()&#10;        }&#10;&#10;        val fingerAngles = mutableMapOf&lt;String, List&lt;Double&gt;&gt;()&#10;&#10;        // Thumb angles (MCP, IP, TIP)&#10;        fingerAngles[&quot;Thumb&quot;] = listOf(&#10;            calculateJointAngle(landmarks[1], landmarks[2], landmarks[3]),  // CMC-MCP-IP&#10;            calculateJointAngle(landmarks[2], landmarks[3], landmarks[4])   // MCP-IP-TIP&#10;        )&#10;&#10;        // Index finger angles&#10;        fingerAngles[&quot;Index&quot;] = listOf(&#10;            calculateJointAngle(landmarks[0], landmarks[5], landmarks[6]),  // WRIST-MCP-PIP&#10;            calculateJointAngle(landmarks[5], landmarks[6], landmarks[7]),  // MCP-PIP-DIP&#10;            calculateJointAngle(landmarks[6], landmarks[7], landmarks[8])   // PIP-DIP-TIP&#10;        )&#10;&#10;        // Middle finger angles&#10;        fingerAngles[&quot;Middle&quot;] = listOf(&#10;            calculateJointAngle(landmarks[0], landmarks[9], landmarks[10]),  // WRIST-MCP-PIP&#10;            calculateJointAngle(landmarks[9], landmarks[10], landmarks[11]), // MCP-PIP-DIP&#10;            calculateJointAngle(landmarks[10], landmarks[11], landmarks[12]) // PIP-DIP-TIP&#10;        )&#10;&#10;        // Ring finger angles&#10;        fingerAngles[&quot;Ring&quot;] = listOf(&#10;            calculateJointAngle(landmarks[0], landmarks[13], landmarks[14]),  // WRIST-MCP-PIP&#10;            calculateJointAngle(landmarks[13], landmarks[14], landmarks[15]), // MCP-PIP-DIP&#10;            calculateJointAngle(landmarks[14], landmarks[15], landmarks[16]) // PIP-DIP-TIP&#10;        )&#10;&#10;        // Pinky finger angles&#10;        fingerAngles[&quot;Pinky&quot;] = listOf(&#10;            calculateJointAngle(landmarks[0], landmarks[17], landmarks[18]),  // WRIST-MCP-PIP&#10;            calculateJointAngle(landmarks[17], landmarks[18], landmarks[19]), // MCP-PIP-DIP&#10;            calculateJointAngle(landmarks[18], landmarks[19], landmarks[20]) // PIP-DIP-TIP&#10;        )&#10;&#10;        return fingerAngles&#10;    }&#10;&#10;    // Calculate a specific finger joint angle&#10;    fun calculateSpecificFingerAngle(&#10;        landmarks: List&lt;NormalizedLandmark&gt;,&#10;        fingerName: String,&#10;        jointIndex: Int&#10;    ): Double? {&#10;        if (landmarks.size &lt; 21) return null&#10;&#10;        return when (fingerName.lowercase()) {&#10;            &quot;thumb&quot; -&gt; when (jointIndex) {&#10;                0 -&gt; calculateJointAngle(landmarks[1], landmarks[2], landmarks[3])&#10;                1 -&gt; calculateJointAngle(landmarks[2], landmarks[3], landmarks[4])&#10;                else -&gt; null&#10;            }&#10;            &quot;index&quot; -&gt; when (jointIndex) {&#10;                0 -&gt; calculateJointAngle(landmarks[0], landmarks[5], landmarks[6])&#10;                1 -&gt; calculateJointAngle(landmarks[5], landmarks[6], landmarks[7])&#10;                2 -&gt; calculateJointAngle(landmarks[6], landmarks[7], landmarks[8])&#10;                else -&gt; null&#10;            }&#10;            &quot;middle&quot; -&gt; when (jointIndex) {&#10;                0 -&gt; calculateJointAngle(landmarks[0], landmarks[9], landmarks[10])&#10;                1 -&gt; calculateJointAngle(landmarks[9], landmarks[10], landmarks[11])&#10;                2 -&gt; calculateJointAngle(landmarks[10], landmarks[11], landmarks[12])&#10;                else -&gt; null&#10;            }&#10;            &quot;ring&quot; -&gt; when (jointIndex) {&#10;                0 -&gt; calculateJointAngle(landmarks[0], landmarks[13], landmarks[14])&#10;                1 -&gt; calculateJointAngle(landmarks[13], landmarks[14], landmarks[15])&#10;                2 -&gt; calculateJointAngle(landmarks[14], landmarks[15], landmarks[16])&#10;                else -&gt; null&#10;            }&#10;            &quot;pinky&quot; -&gt; when (jointIndex) {&#10;                0 -&gt; calculateJointAngle(landmarks[0], landmarks[17], landmarks[18])&#10;                1 -&gt; calculateJointAngle(landmarks[17], landmarks[18], landmarks[19])&#10;                2 -&gt; calculateJointAngle(landmarks[18], landmarks[19], landmarks[20])&#10;                else -&gt; null&#10;            }&#10;            else -&gt; null&#10;        }&#10;    }&#10;&#10;    companion object {&#10;        val TAG = &quot;GestureRecognizerHelper ${this.hashCode()}&quot;&#10;        private const val MP_RECOGNIZER_TASK = &quot;gesture_recognizer.task&quot;&#10;&#10;        const val DELEGATE_CPU = 0&#10;        const val DELEGATE_GPU = 1&#10;        const val DEFAULT_HAND_DETECTION_CONFIDENCE = 0.5F&#10;        const val DEFAULT_HAND_TRACKING_CONFIDENCE = 0.5F&#10;        const val DEFAULT_HAND_PRESENCE_CONFIDENCE = 0.5F&#10;        const val OTHER_ERROR = 0&#10;        const val GPU_ERROR = 1&#10;    }&#10;&#10;    data class ResultBundle(&#10;        val results: List&lt;GestureRecognizerResult&gt;,&#10;        val inferenceTime: Long,&#10;        val inputImageHeight: Int,&#10;        val inputImageWidth: Int,&#10;    )&#10;&#10;    interface GestureRecognizerListener {&#10;        fun onError(error: String, errorCode: Int = OTHER_ERROR)&#10;        fun onResults(resultBundle: ResultBundle)&#10;    }&#10;}&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>